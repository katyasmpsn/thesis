{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../results/cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"../results/cleaned_data_jan24.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../data/hydrated_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# input_type = sys.argv[1]\n",
    "# in_file = sys.argv[2]\n",
    "\n",
    "input_type = \"tweets\"\n",
    "in_file = \"../embeddings124/tweet_embeddings_jan24.csv\"\n",
    "\n",
    "if input_type == \"tweets\":\n",
    "    weight_file = \"../results/tweet_vocab_counts.csv\"\n",
    "elif input_type == \"notes\":\n",
    "    weight_file = \"../results/note_vocab_counts.csv\"\n",
    "else:\n",
    "    print(\"Incorrect argument! Please use `tweets` or `notes`\")\n",
    "\n",
    "# reading in embeddings\n",
    "# df = pd.read_csv(\"results/embeddings/note_embeddings_jan21.csv\", index_col=False)\n",
    "df = pd.read_csv(in_file, index_col=False)\n",
    "# not sure why this column exists\n",
    "# df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "word_type_list = df.word.tolist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>dim_0</th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "      <th>...</th>\n",
       "      <th>dim_758</th>\n",
       "      <th>dim_759</th>\n",
       "      <th>dim_760</th>\n",
       "      <th>dim_761</th>\n",
       "      <th>dim_762</th>\n",
       "      <th>dim_763</th>\n",
       "      <th>dim_764</th>\n",
       "      <th>dim_765</th>\n",
       "      <th>dim_766</th>\n",
       "      <th>dim_767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>-0.366456</td>\n",
       "      <td>-0.039459</td>\n",
       "      <td>0.066328</td>\n",
       "      <td>0.145614</td>\n",
       "      <td>0.143236</td>\n",
       "      <td>-0.256089</td>\n",
       "      <td>0.260212</td>\n",
       "      <td>0.613425</td>\n",
       "      <td>-0.210487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087990</td>\n",
       "      <td>0.183494</td>\n",
       "      <td>0.013462</td>\n",
       "      <td>-0.249022</td>\n",
       "      <td>-0.026073</td>\n",
       "      <td>-0.109739</td>\n",
       "      <td>0.140210</td>\n",
       "      <td>-0.311264</td>\n",
       "      <td>0.084637</td>\n",
       "      <td>0.532847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a9</td>\n",
       "      <td>-0.138435</td>\n",
       "      <td>-0.281672</td>\n",
       "      <td>1.115078</td>\n",
       "      <td>0.014905</td>\n",
       "      <td>0.361609</td>\n",
       "      <td>-0.028945</td>\n",
       "      <td>-0.335671</td>\n",
       "      <td>0.574793</td>\n",
       "      <td>0.169485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031248</td>\n",
       "      <td>0.073497</td>\n",
       "      <td>0.424837</td>\n",
       "      <td>-0.721684</td>\n",
       "      <td>0.459409</td>\n",
       "      <td>0.452788</td>\n",
       "      <td>0.527045</td>\n",
       "      <td>-0.075617</td>\n",
       "      <td>-0.078410</td>\n",
       "      <td>0.131503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aal</td>\n",
       "      <td>-0.231705</td>\n",
       "      <td>0.469943</td>\n",
       "      <td>0.135738</td>\n",
       "      <td>-0.352974</td>\n",
       "      <td>0.218890</td>\n",
       "      <td>-0.560826</td>\n",
       "      <td>-0.155215</td>\n",
       "      <td>-0.406691</td>\n",
       "      <td>0.118872</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.689072</td>\n",
       "      <td>-0.019370</td>\n",
       "      <td>-0.343725</td>\n",
       "      <td>-0.024113</td>\n",
       "      <td>-0.051281</td>\n",
       "      <td>0.389565</td>\n",
       "      <td>-0.288845</td>\n",
       "      <td>0.098972</td>\n",
       "      <td>0.534160</td>\n",
       "      <td>-0.025546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aap</td>\n",
       "      <td>-0.025668</td>\n",
       "      <td>0.031731</td>\n",
       "      <td>-0.037452</td>\n",
       "      <td>-0.268505</td>\n",
       "      <td>0.234108</td>\n",
       "      <td>-0.539271</td>\n",
       "      <td>0.087068</td>\n",
       "      <td>-0.030476</td>\n",
       "      <td>-0.597631</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.375702</td>\n",
       "      <td>0.208157</td>\n",
       "      <td>-0.557366</td>\n",
       "      <td>-0.263982</td>\n",
       "      <td>-0.167136</td>\n",
       "      <td>-0.058155</td>\n",
       "      <td>-0.057982</td>\n",
       "      <td>-0.168027</td>\n",
       "      <td>0.292943</td>\n",
       "      <td>0.136588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aapi</td>\n",
       "      <td>0.010717</td>\n",
       "      <td>-0.246908</td>\n",
       "      <td>0.074198</td>\n",
       "      <td>-0.630390</td>\n",
       "      <td>0.510388</td>\n",
       "      <td>0.284703</td>\n",
       "      <td>0.252908</td>\n",
       "      <td>0.064541</td>\n",
       "      <td>-0.555568</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.728646</td>\n",
       "      <td>0.400712</td>\n",
       "      <td>-0.534700</td>\n",
       "      <td>-0.077891</td>\n",
       "      <td>0.129538</td>\n",
       "      <td>0.485169</td>\n",
       "      <td>-0.070227</td>\n",
       "      <td>-0.225387</td>\n",
       "      <td>0.318419</td>\n",
       "      <td>0.317413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word     dim_0     dim_1     dim_2     dim_3     dim_4     dim_5     dim_6  \\\n",
       "0     a -0.366456 -0.039459  0.066328  0.145614  0.143236 -0.256089  0.260212   \n",
       "1    a9 -0.138435 -0.281672  1.115078  0.014905  0.361609 -0.028945 -0.335671   \n",
       "2   aal -0.231705  0.469943  0.135738 -0.352974  0.218890 -0.560826 -0.155215   \n",
       "3   aap -0.025668  0.031731 -0.037452 -0.268505  0.234108 -0.539271  0.087068   \n",
       "4  aapi  0.010717 -0.246908  0.074198 -0.630390  0.510388  0.284703  0.252908   \n",
       "\n",
       "      dim_7     dim_8  ...   dim_758   dim_759   dim_760   dim_761   dim_762  \\\n",
       "0  0.613425 -0.210487  ...  0.087990  0.183494  0.013462 -0.249022 -0.026073   \n",
       "1  0.574793  0.169485  ...  0.031248  0.073497  0.424837 -0.721684  0.459409   \n",
       "2 -0.406691  0.118872  ... -0.689072 -0.019370 -0.343725 -0.024113 -0.051281   \n",
       "3 -0.030476 -0.597631  ... -0.375702  0.208157 -0.557366 -0.263982 -0.167136   \n",
       "4  0.064541 -0.555568  ... -0.728646  0.400712 -0.534700 -0.077891  0.129538   \n",
       "\n",
       "    dim_763   dim_764   dim_765   dim_766   dim_767  \n",
       "0 -0.109739  0.140210 -0.311264  0.084637  0.532847  \n",
       "1  0.452788  0.527045 -0.075617 -0.078410  0.131503  \n",
       "2  0.389565 -0.288845  0.098972  0.534160 -0.025546  \n",
       "3 -0.058155 -0.057982 -0.168027  0.292943  0.136588  \n",
       "4  0.485169 -0.070227 -0.225387  0.318419  0.317413  \n",
       "\n",
       "[5 rows x 769 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1:\n",
    "Initial PCA reduction. Sia et al found that KM allows for dimension reduction of up to 80%.\n",
    "In Sia et al, this dimension reduction was primarily to reduce clustering complexity.\n",
    "Not sure if the same logic applies for this exercise.\n",
    "This should be tested as a hyperparameter; but we can start by going from 768 -> 100\n",
    "\"\"\"\n",
    "\n",
    "dims = 100\n",
    "pca_100d = PCA(n_components=dims)\n",
    "X = pd.DataFrame(pca_100d.fit_transform(df.drop(columns=[\"word\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 2:\n",
    "KMeans with N topics (centroids). Again, topics might be used a hyperparameter\n",
    "Setting N = 20 for some initial testing\n",
    "\"\"\"\n",
    "\n",
    "# based on https://www.kaggle.com/minc33/visualizing-high-dimensional-clusters\n",
    "\n",
    "X[\"word_type\"] = word_type_list\n",
    "weights = pd.read_csv(weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>word_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.922736</td>\n",
       "      <td>-1.157409</td>\n",
       "      <td>-1.003836</td>\n",
       "      <td>-0.054212</td>\n",
       "      <td>-0.197361</td>\n",
       "      <td>0.670005</td>\n",
       "      <td>0.107601</td>\n",
       "      <td>0.081208</td>\n",
       "      <td>-0.189267</td>\n",
       "      <td>-0.496182</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075690</td>\n",
       "      <td>-0.009870</td>\n",
       "      <td>0.056770</td>\n",
       "      <td>-0.044893</td>\n",
       "      <td>0.096636</td>\n",
       "      <td>0.013212</td>\n",
       "      <td>-0.072024</td>\n",
       "      <td>-0.057799</td>\n",
       "      <td>-0.081137</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.446454</td>\n",
       "      <td>1.194072</td>\n",
       "      <td>-0.340779</td>\n",
       "      <td>-1.434484</td>\n",
       "      <td>-0.804683</td>\n",
       "      <td>-0.083489</td>\n",
       "      <td>-0.526569</td>\n",
       "      <td>0.224787</td>\n",
       "      <td>-0.417680</td>\n",
       "      <td>0.295917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060286</td>\n",
       "      <td>-0.140361</td>\n",
       "      <td>0.620122</td>\n",
       "      <td>0.068010</td>\n",
       "      <td>-0.172855</td>\n",
       "      <td>-0.674359</td>\n",
       "      <td>0.417426</td>\n",
       "      <td>-0.561370</td>\n",
       "      <td>-0.952213</td>\n",
       "      <td>a9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.961884</td>\n",
       "      <td>2.091798</td>\n",
       "      <td>-0.440215</td>\n",
       "      <td>-0.365224</td>\n",
       "      <td>-2.071579</td>\n",
       "      <td>-0.032496</td>\n",
       "      <td>-0.167538</td>\n",
       "      <td>-0.603179</td>\n",
       "      <td>0.547061</td>\n",
       "      <td>-0.042937</td>\n",
       "      <td>...</td>\n",
       "      <td>0.749088</td>\n",
       "      <td>-0.008035</td>\n",
       "      <td>0.693091</td>\n",
       "      <td>0.060770</td>\n",
       "      <td>-0.770682</td>\n",
       "      <td>0.568991</td>\n",
       "      <td>-0.128693</td>\n",
       "      <td>0.403753</td>\n",
       "      <td>0.409133</td>\n",
       "      <td>aal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.312245</td>\n",
       "      <td>1.551615</td>\n",
       "      <td>-0.524371</td>\n",
       "      <td>-1.748499</td>\n",
       "      <td>-2.350163</td>\n",
       "      <td>1.431939</td>\n",
       "      <td>1.230254</td>\n",
       "      <td>-0.734550</td>\n",
       "      <td>0.881604</td>\n",
       "      <td>1.600390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629105</td>\n",
       "      <td>-0.216808</td>\n",
       "      <td>0.280428</td>\n",
       "      <td>-0.064276</td>\n",
       "      <td>0.281123</td>\n",
       "      <td>0.868219</td>\n",
       "      <td>-0.177705</td>\n",
       "      <td>0.541364</td>\n",
       "      <td>-0.055041</td>\n",
       "      <td>aap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.078814</td>\n",
       "      <td>2.077811</td>\n",
       "      <td>-0.462314</td>\n",
       "      <td>-0.279202</td>\n",
       "      <td>-1.232848</td>\n",
       "      <td>-0.956169</td>\n",
       "      <td>1.257564</td>\n",
       "      <td>-1.094511</td>\n",
       "      <td>3.178278</td>\n",
       "      <td>1.108443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.679141</td>\n",
       "      <td>0.150271</td>\n",
       "      <td>0.476826</td>\n",
       "      <td>0.674819</td>\n",
       "      <td>0.160347</td>\n",
       "      <td>-0.147838</td>\n",
       "      <td>0.512065</td>\n",
       "      <td>0.613124</td>\n",
       "      <td>0.023689</td>\n",
       "      <td>aapi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18609</th>\n",
       "      <td>-0.491577</td>\n",
       "      <td>0.218695</td>\n",
       "      <td>1.078646</td>\n",
       "      <td>-0.917450</td>\n",
       "      <td>-0.132998</td>\n",
       "      <td>1.263813</td>\n",
       "      <td>-1.559672</td>\n",
       "      <td>-1.053036</td>\n",
       "      <td>-0.107665</td>\n",
       "      <td>-1.140574</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.713136</td>\n",
       "      <td>-0.204917</td>\n",
       "      <td>0.503512</td>\n",
       "      <td>-0.304067</td>\n",
       "      <td>0.674400</td>\n",
       "      <td>0.467876</td>\n",
       "      <td>0.824119</td>\n",
       "      <td>-0.818583</td>\n",
       "      <td>-0.750706</td>\n",
       "      <td>zoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18610</th>\n",
       "      <td>-0.878050</td>\n",
       "      <td>1.422073</td>\n",
       "      <td>-0.409335</td>\n",
       "      <td>-2.688034</td>\n",
       "      <td>-0.350826</td>\n",
       "      <td>0.387051</td>\n",
       "      <td>1.072441</td>\n",
       "      <td>-1.241842</td>\n",
       "      <td>-1.064466</td>\n",
       "      <td>-0.993255</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.212577</td>\n",
       "      <td>-0.550917</td>\n",
       "      <td>0.497018</td>\n",
       "      <td>0.315895</td>\n",
       "      <td>0.086716</td>\n",
       "      <td>0.253532</td>\n",
       "      <td>0.374564</td>\n",
       "      <td>-0.396250</td>\n",
       "      <td>-0.561489</td>\n",
       "      <td>zoster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18611</th>\n",
       "      <td>3.232246</td>\n",
       "      <td>0.622550</td>\n",
       "      <td>-0.473562</td>\n",
       "      <td>-2.752281</td>\n",
       "      <td>-1.045740</td>\n",
       "      <td>-0.690028</td>\n",
       "      <td>-0.512607</td>\n",
       "      <td>-1.503498</td>\n",
       "      <td>-0.116293</td>\n",
       "      <td>-1.414615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.209096</td>\n",
       "      <td>-0.605523</td>\n",
       "      <td>0.429745</td>\n",
       "      <td>0.310884</td>\n",
       "      <td>0.545161</td>\n",
       "      <td>0.287593</td>\n",
       "      <td>0.038161</td>\n",
       "      <td>-0.218427</td>\n",
       "      <td>0.105710</td>\n",
       "      <td>zpack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18612</th>\n",
       "      <td>-2.961886</td>\n",
       "      <td>3.431228</td>\n",
       "      <td>-2.236192</td>\n",
       "      <td>1.604210</td>\n",
       "      <td>-0.907618</td>\n",
       "      <td>-0.430854</td>\n",
       "      <td>-0.630263</td>\n",
       "      <td>-0.027643</td>\n",
       "      <td>0.355928</td>\n",
       "      <td>0.169294</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000475</td>\n",
       "      <td>-0.266757</td>\n",
       "      <td>0.709839</td>\n",
       "      <td>-0.096957</td>\n",
       "      <td>0.093321</td>\n",
       "      <td>0.062808</td>\n",
       "      <td>-0.424486</td>\n",
       "      <td>0.486418</td>\n",
       "      <td>-0.563172</td>\n",
       "      <td>zuckerberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18613</th>\n",
       "      <td>-3.336139</td>\n",
       "      <td>2.330883</td>\n",
       "      <td>0.990980</td>\n",
       "      <td>-2.332718</td>\n",
       "      <td>-0.290872</td>\n",
       "      <td>-0.627931</td>\n",
       "      <td>0.938712</td>\n",
       "      <td>0.560334</td>\n",
       "      <td>-1.132142</td>\n",
       "      <td>0.121634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074997</td>\n",
       "      <td>-0.661918</td>\n",
       "      <td>-1.103761</td>\n",
       "      <td>0.260639</td>\n",
       "      <td>0.358689</td>\n",
       "      <td>-0.503885</td>\n",
       "      <td>-0.241344</td>\n",
       "      <td>-0.104531</td>\n",
       "      <td>-0.499886</td>\n",
       "      <td>zygote</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18614 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0     -1.922736 -1.157409 -1.003836 -0.054212 -0.197361  0.670005  0.107601   \n",
       "1      1.446454  1.194072 -0.340779 -1.434484 -0.804683 -0.083489 -0.526569   \n",
       "2      0.961884  2.091798 -0.440215 -0.365224 -2.071579 -0.032496 -0.167538   \n",
       "3     -0.312245  1.551615 -0.524371 -1.748499 -2.350163  1.431939  1.230254   \n",
       "4      0.078814  2.077811 -0.462314 -0.279202 -1.232848 -0.956169  1.257564   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "18609 -0.491577  0.218695  1.078646 -0.917450 -0.132998  1.263813 -1.559672   \n",
       "18610 -0.878050  1.422073 -0.409335 -2.688034 -0.350826  0.387051  1.072441   \n",
       "18611  3.232246  0.622550 -0.473562 -2.752281 -1.045740 -0.690028 -0.512607   \n",
       "18612 -2.961886  3.431228 -2.236192  1.604210 -0.907618 -0.430854 -0.630263   \n",
       "18613 -3.336139  2.330883  0.990980 -2.332718 -0.290872 -0.627931  0.938712   \n",
       "\n",
       "              7         8         9  ...        91        92        93  \\\n",
       "0      0.081208 -0.189267 -0.496182  ... -0.075690 -0.009870  0.056770   \n",
       "1      0.224787 -0.417680  0.295917  ...  0.060286 -0.140361  0.620122   \n",
       "2     -0.603179  0.547061 -0.042937  ...  0.749088 -0.008035  0.693091   \n",
       "3     -0.734550  0.881604  1.600390  ...  0.629105 -0.216808  0.280428   \n",
       "4     -1.094511  3.178278  1.108443  ...  0.679141  0.150271  0.476826   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "18609 -1.053036 -0.107665 -1.140574  ... -0.713136 -0.204917  0.503512   \n",
       "18610 -1.241842 -1.064466 -0.993255  ... -0.212577 -0.550917  0.497018   \n",
       "18611 -1.503498 -0.116293 -1.414615  ...  0.209096 -0.605523  0.429745   \n",
       "18612 -0.027643  0.355928  0.169294  ... -0.000475 -0.266757  0.709839   \n",
       "18613  0.560334 -1.132142  0.121634  ...  0.074997 -0.661918 -1.103761   \n",
       "\n",
       "             94        95        96        97        98        99   word_type  \n",
       "0     -0.044893  0.096636  0.013212 -0.072024 -0.057799 -0.081137           a  \n",
       "1      0.068010 -0.172855 -0.674359  0.417426 -0.561370 -0.952213          a9  \n",
       "2      0.060770 -0.770682  0.568991 -0.128693  0.403753  0.409133         aal  \n",
       "3     -0.064276  0.281123  0.868219 -0.177705  0.541364 -0.055041         aap  \n",
       "4      0.674819  0.160347 -0.147838  0.512065  0.613124  0.023689        aapi  \n",
       "...         ...       ...       ...       ...       ...       ...         ...  \n",
       "18609 -0.304067  0.674400  0.467876  0.824119 -0.818583 -0.750706        zoos  \n",
       "18610  0.315895  0.086716  0.253532  0.374564 -0.396250 -0.561489      zoster  \n",
       "18611  0.310884  0.545161  0.287593  0.038161 -0.218427  0.105710       zpack  \n",
       "18612 -0.096957  0.093321  0.062808 -0.424486  0.486418 -0.563172  zuckerberg  \n",
       "18613  0.260639  0.358689 -0.503885 -0.241344 -0.104531 -0.499886      zygote  \n",
       "\n",
       "[18614 rows x 101 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-19c75d6c40bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mn_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/573env/lib/python3.6/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m         \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m# Validate init array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/573env/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_check_sample_weight\u001b[0;34m(sample_weight, X, dtype)\u001b[0m\n\u001b[1;32m   1316\u001b[0m         sample_weight = check_array(\n\u001b[1;32m   1317\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1318\u001b[0;31m             \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1319\u001b[0m         )\n\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/573env/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/573env/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 664\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/573env/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     (type_err,\n\u001b[0;32m--> 106\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m    107\u001b[0m             )\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "\n",
    "weights.columns = [\"word_type\", \"tf\"]\n",
    "X = pd.merge(X, weights, how=\"left\", on=\"word_type\")\n",
    "\n",
    "word_type = X[\"word_type\"].to_list()\n",
    "weights = X[\"tf\"].to_list()\n",
    "X = X.drop(columns = [\"word_type\", \"tf\"])\n",
    "\n",
    "n_clusters = 20\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "kmeans.fit(X, sample_weight=weights)\n",
    "clusters = kmeans.predict(X)\n",
    "\n",
    "X[\"Cluster\"] = clusters\n",
    "X[\"Word_Type\"] = word_type\n",
    "X[\"Weights\"] = weights\n",
    "\n",
    "# out_file = \"results/clusters_{0}_{1}_{2}_{3}\".format(input_type, dims, n_clusters, pd.Timestamp.today().strftime(\"%m_%d\")\n",
    "\n",
    "# )\n",
    "# of = open(out_file, 'w')\n",
    "# X[[\"Cluster\",\"Word_Type\",\"Weights\"]].to_csv(of)\n",
    "# of.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word_Type</th>\n",
       "      <th>Weights</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[abandon, abducted, abetted, abstain, abuse, a...</td>\n",
       "      <td>713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[aapi, abolitionists, abstract, activism, acti...</td>\n",
       "      <td>588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[academic, academics, according, accountant, a...</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[admn, af1, ageless, alissa, am, anyways, appr...</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[about, accept, accepts, accordingly, accusati...</td>\n",
       "      <td>880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[a9, aap, absence, absorption, academy, accele...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[assn, big, bots, breakdowns, canadas, chills,...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[aba, aca, act, ada, administration, administr...</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[accidents, accounts, achievements, acids, act...</td>\n",
       "      <td>1029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[barack, bernie, buchanan, bush, carter, chave...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[accelerates, accuses, adheres, adopts, aims, ...</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[buzzwords, caveat, clickbait, downthread, has...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[a, abnormally, abruptly, actually, addition, ...</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[abilities, abortionist, abortionprocedurescom...</td>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[abolished, abolition, abundance, accelerate, ...</td>\n",
       "      <td>782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[abnormality, aborted, abortion, abortions, ab...</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[abbreviated, absent, absolutely, accordance, ...</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[aal, aaron, aarons, aarp, aay, ab, abbot, abb...</td>\n",
       "      <td>2029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[absentees, adolescents, adult, adults, afghan...</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[abort, absorbed, adenovirus, administered, ad...</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[aas, aboardi, abt, adizs, agains, aint, allma...</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[absentee, ambitions, ballot, ballotbymail, ba...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[abc, accession, afb, afghan, afghanistan, afh...</td>\n",
       "      <td>616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[above, accessed, accompanied, accompanying, a...</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[acip, acog, activeduty, ade, adhd, afib, ages...</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[absurd, accuracy, accurate, accurately, affai...</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[abada, abbas, abraham, abu, accenture, accord...</td>\n",
       "      <td>1396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[accounting, accumulated, accumulation, adbase...</td>\n",
       "      <td>608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[abandoned, ability, able, absolute, accepted,...</td>\n",
       "      <td>1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[abhorrent, abide, abject, abolish, abolishing...</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Word_Type  Weights\n",
       "Cluster                                                            \n",
       "0        [abandon, abducted, abetted, abstain, abuse, a...      713\n",
       "1        [aapi, abolitionists, abstract, activism, acti...      588\n",
       "2        [academic, academics, according, accountant, a...      258\n",
       "3        [admn, af1, ageless, alissa, am, anyways, appr...      119\n",
       "4        [about, accept, accepts, accordingly, accusati...      880\n",
       "5        [a9, aap, absence, absorption, academy, accele...     2016\n",
       "6        [assn, big, bots, breakdowns, canadas, chills,...       30\n",
       "7        [aba, aca, act, ada, administration, administr...      219\n",
       "8        [accidents, accounts, achievements, acids, act...     1029\n",
       "9        [barack, bernie, buchanan, bush, carter, chave...       49\n",
       "10       [accelerates, accuses, adheres, adopts, aims, ...      192\n",
       "11       [buzzwords, caveat, clickbait, downthread, has...       38\n",
       "12       [a, abnormally, abruptly, actually, addition, ...      253\n",
       "13       [abilities, abortionist, abortionprocedurescom...      670\n",
       "14       [abolished, abolition, abundance, accelerate, ...      782\n",
       "15       [abnormality, aborted, abortion, abortions, ab...      410\n",
       "16       [abbreviated, absent, absolutely, accordance, ...      389\n",
       "17       [aal, aaron, aarons, aarp, aay, ab, abbot, abb...     2029\n",
       "18       [absentees, adolescents, adult, adults, afghan...      288\n",
       "19       [abort, absorbed, adenovirus, administered, ad...      168\n",
       "20       [aas, aboardi, abt, adizs, agains, aint, allma...      244\n",
       "21       [absentee, ambitions, ballot, ballotbymail, ba...       95\n",
       "22       [abc, accession, afb, afghan, afghanistan, afh...      616\n",
       "23       [above, accessed, accompanied, accompanying, a...      577\n",
       "24       [acip, acog, activeduty, ade, adhd, afib, ages...      393\n",
       "25       [absurd, accuracy, accurate, accurately, affai...      309\n",
       "26       [abada, abbas, abraham, abu, accenture, accord...     1396\n",
       "27       [accounting, accumulated, accumulation, adbase...      608\n",
       "28       [abandoned, ability, able, absolute, accepted,...     1265\n",
       "29       [abhorrent, abide, abject, abolish, abolishing...     1991"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.groupby(\"Cluster\").agg({\"Word_Type\": list, \"Weights\": \"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (573group)",
   "language": "python",
   "name": "pycharm-4c1ba1ad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
