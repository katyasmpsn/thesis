{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../results/test.csv\") # reading in the test data to create a word-word freq matrix \n",
    "clusters = pd.read_csv(\"../results/all_runs.csv\") # reading in the top words for each cluster (with hyperparameter variations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['noteLength'] = df['noteText'].str.split().str.len()\n",
    "df['tweetLength'] = df['tweetText'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: make this into a log \n",
    "df['tweetLength'].quantile(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 14\n",
    "# the window should be longer for semantics. The first quartile of note length is 14 words, for both tweets and notes, \n",
    "# so I'll go with that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# combining note and tweet text into a single list \n",
    "\n",
    "note_list = df.noteTextList.tolist() \n",
    "tweet_list = df.tweetTextList.tolist()\n",
    "\n",
    "note_list = [literal_eval(x) for x in note_list]\n",
    "tweet_list = [literal_eval(x) for x in tweet_list]\n",
    "\n",
    "\n",
    "word_list = note_list + tweet_list\n",
    "flat_word_list = [item for sublist in word_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need a preprocessing step where if a word in the clusters isn't found in the test dataset, it's omitted \n",
    "# because that word would have no frequency statistics associated with it \n",
    "clusters['Test_Train_Match'] = np.where(clusters['Word_Type'].isin(flat_word_list), 1, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    18782\n",
       "0     1081\n",
       "Name: Test_Train_Match, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: log this!! omitting about 1081 words by my count \n",
    "clusters['Test_Train_Match'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just going to omit them because idk \n",
    "clusters = clusters[clusters['Test_Train_Match'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_neighborhood_cts(word_list, i, window):\n",
    "    \"\"\"\n",
    "    Helper function to calculate NPMI statistics for the given window\n",
    "    INPUT: a list of tokens, a pointer for the key word, window size\n",
    "    OUTPUT: a dictionary of counts for the neighborhood of the key word\n",
    "    \"\"\"\n",
    "    pre_pane_li = i-window\n",
    "    pre_pane_ri = i\n",
    "    post_pane_li = i+1\n",
    "    post_pane_ri = i+1+window\n",
    "    if pre_pane_li < window  and i < window:\n",
    "        pre_pane_li = 0\n",
    "    if i == len(word_list) - 1:\n",
    "        pre_pane_li = len(word_list) - window - 1\n",
    "    neighborhood = word_list[pre_pane_li:pre_pane_ri] + word_list[post_pane_li:post_pane_ri]\n",
    "    inner_vecs = dict.fromkeys(set(neighborhood), 0)\n",
    "    for word in neighborhood:\n",
    "        inner_vecs[word] = inner_vecs[word] + 1\n",
    "    return inner_vecs\n",
    "\n",
    "\n",
    "def mainPMIStats(master_class, word_list)\n",
    "master_class = {}\n",
    "total_ct = 0\n",
    "for l in word_list:\n",
    "    word_list = l\n",
    "    outer_vec = {}\n",
    "    for i in range(len(word_list)): # Looping thru all words\n",
    "        tmp = return_neighborhood_cts(word_list, i, window) # returning neighborhood counts for a key word\n",
    "        if (word_list[i] in outer_vec.keys()): # if a key word already has a dict entry, add to it\n",
    "            for k, v in tmp.items():\n",
    "                total_ct += v\n",
    "                try:\n",
    "                    outer_vec[word_list[i]][k] = outer_vec[word_list[i]][k] + v\n",
    "                except:\n",
    "                    outer_vec[word_list[i]][k] = v\n",
    "        else: # if a key word does not have a dict entry, create it\n",
    "            outer_vec[word_list[i]] = tmp\n",
    "            total_ct += sum(tmp.values())\n",
    "            \n",
    "    # updating the master dict over many documents\n",
    "    for k,v in outer_vec.items():\n",
    "        try:\n",
    "            master_class[k] = Counter(master_class[k]) + Counter(outer_vec[k])\n",
    "        except:\n",
    "            master_class[k] = outer_vec[k]\n",
    "    \n",
    "\n",
    "# make every dict in the master dictionary a counter object such that when a co-occurence doesn't occur, the joint \n",
    "# probability == 0 \n",
    "\n",
    "master_class = {key: Counter(value) for key, value in master_class.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npmi(w1, w2, vectors, vector_ct):\n",
    "    # INPUT two words to compare, dict of vectors, total word count for denom\n",
    "    # OUTPUT ppmi for those two words\n",
    "    eps = 10**(-12)\n",
    "    # numerator\n",
    "    w1w2_dc = vectors[w1][w2] / vector_ct\n",
    "    w1_dc = sum(vectors[w1].values()) / vector_ct\n",
    "    w2_dc = sum(vectors[w2].values()) / vector_ct\n",
    "    \n",
    "    pmi_w1w2 = np.log((w1w2_dc) / ((w1_dc * w2_dc) + eps) + eps)\n",
    "    npmi_w1w2 = pmi_w1w2 / (- np.log( (w1w2_dc) + eps))\n",
    "    return npmi_w1w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgClusterNPMI(cluster_words, stats):\n",
    "    \"\"\"\n",
    "    Input: List of words in a cluster\n",
    "    Output: Average NPMI for the cluster \n",
    "    wtf is happening? It is averaged twice: NPMI is calculated for each w1, w2; so an average of all the pairs is taken\n",
    "    And, an average of all of the words is taken in the cluster.\n",
    "    It looks like Lau et al sums the NPMIs but that would result in NPMIs that aren't really interpretable (not\n",
    "    within the [-1,1] range)\n",
    "    \"\"\"\n",
    "\n",
    "    npmi_scores = {}\n",
    "    for w1 in cluster_words:\n",
    "        npmi_sum = 0\n",
    "        ct = 0\n",
    "        for w2 in cluster_words:\n",
    "            if w1 != w2:\n",
    "                res = npmi(w1, w2, stats, total_ct)\n",
    "#                 print(w1,w2,res)\n",
    "#                 print(w1, w2, res)\n",
    "                \n",
    "                npmi_sum += res\n",
    "                ct += 1\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "        # taking the average of every w1 against every other w2\n",
    "        npmi_scores[w1] = npmi_sum/ct\n",
    "       \n",
    "    \n",
    "    res = 0\n",
    "    for val in npmi_scores.values():\n",
    "        \n",
    "        res += val\n",
    "        \n",
    "  \n",
    "    # using len() to get total keys for mean computation\n",
    "    # averaging all words in a cluster \n",
    "\n",
    "    res = res / len(npmi_scores)\n",
    "\n",
    "    \n",
    "    return (res, npmi_scores)4       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgClusterNPMI([\"biden\", \"putin\", \"russia\", \"usa\"], master_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = clusters[(clusters['Cluster'] == 0 ) &(clusters['numCluster'] == 20 ) & (clusters['dimensions'] == 100)\n",
    "        & (clusters['seed'] == 932)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = temp.Word_Type.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgClusterNPMI(lst, master_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clusters.groupby(['numCluster', 'dimensions', 'seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data = []\n",
    "omitted_clusters = []\n",
    "for key, item in df:\n",
    "    print(key)\n",
    "    # iterating through each dataframe of results for each set of hyperparamaters \n",
    "    cluster_ids = item.Cluster.unique().tolist()\n",
    "    cluster_scores = 0\n",
    "    for cluster_id in cluster_ids:\n",
    "#         print(cluster_id)\n",
    "        temp_df = item[item['Cluster'] == cluster_id]\n",
    "        word_list = temp_df.Word_Type.tolist() \n",
    "        if len(word_list) != 1:\n",
    "            score = avgClusterNPMI(word_list, master_class)\n",
    "            cluster_scores += score\n",
    "        else:\n",
    "            omitted_clusters.append((key,cluster_id))\n",
    "            # some clusters might only have one word. They're omitted from the analysis\n",
    "            continue\n",
    "    run_score = cluster_scores/len(cluster_ids)\n",
    "    run_data.append([key[0], key[1], key[2], run_score])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(run_data, columns=['NClusters', 'NDims', 'RandomSeed', \"Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results.groupby(['NClusters','NDims'])['Score'].mean()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = clusters.groupby(['Cluster']).agg({\"Word_Type\": list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ids = len(clusters)\n",
    "\n",
    "\n",
    "avg_npmis = []\n",
    "for i in range(cluster_ids):\n",
    "    cluster_words = clusters.iloc[i]['Word_Type']\n",
    "\n",
    "    # this portion calculates the average average NPMI over a cluster \n",
    "    npmi_scores = {}\n",
    "    for w1 in cluster_words:\n",
    "        npmi_sum = 0\n",
    "        ct = 0\n",
    "        for w2 in cluster_words:\n",
    "            if w1 != w2:\n",
    "                res = npmi(w1, w2, master_class, total_ct)\n",
    "    #             print(w1, w2, res)\n",
    "                npmi_sum += res\n",
    "                ct += 1\n",
    "            else:\n",
    "                pass\n",
    "        # taking the average of every w1 against every other w2\n",
    "        try:\n",
    "            npmi_scores[w1] = npmi_sum/ct\n",
    "        except:\n",
    "            \n",
    "    \n",
    "    res = 0\n",
    "    for val in npmi_scores.values():\n",
    "        res += val\n",
    "  \n",
    "    # using len() to get total keys for mean computation\n",
    "    # averaging all words in a cluster \n",
    "    res = res / len(npmi_scores)\n",
    "    \n",
    "    final = (i,res)\n",
    "    avg_npmis.append(np.around((final),5))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_npmis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_npmis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}